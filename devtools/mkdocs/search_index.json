{
    "docs": [
        {
            "location": "/",
            "text": "DevTools\n\n\nThis project is designed to creates a Docker Machine based VM, sets up network routes, \na persistent /data store, and runs a DNS service for Docker containers.\n\n\nIf you are brand new to using the DevTools VM it's \nhighly recommended\n\nto read the \nIntro\n document (see the docs directory) for a conceptual overview.\nThere is additional documentation in the docs directory that guides you through \na variety of topics, including project and integration environment setup.  \n\n\nInstallation\n\n\nAs first time setup, install the following:\n\n\n1. Install VirtualBox \n= 5.0 if you aren't on Linux\n\n\nVirtualBox website\n\n\n2. Install devtools on Mac using Homebrew\n\n\n2.1 Install the devtools Tap\n\n\nbrew tap phase2/devtools\n\n\n2.2 Install the devtools binary and dependencies\n\n\nbrew install phase2/devtools/docker\n\n\nbrew install phase2/devtools/docker-machine\n\n\nbrew install phase2/devtools/docker-compose\n\n\nbrew install devtools\n\n\n2.3 Verify the proper devtools environment\n\n\nRun \ndevtools doctor\n to ensure the proper versions of the Docker binaries are available\n\n\n2.4 Startup the container host\n\n\nOnce everything checks out. run the following command to create \n start the new container \nhost (the docker machine). (You will be prompted for your admin password)\n\n\ndevtools start\n\n\n2.5 Configure your shell to use Dev Tools environment\n\n\nTo configure the shell with the proper Dev Tools environment, run the following command\n\n\neval \"$(devtools config)\"\n\n\nTo make this permanent on every terminal you launch, add the following to your \n   .bash_profile, .zshrc or equivalent:\n\n\n# Support for Dev Tools\n   eval \"$(devtools config)\"",
            "title": "Home"
        },
        {
            "location": "/#devtools",
            "text": "This project is designed to creates a Docker Machine based VM, sets up network routes, \na persistent /data store, and runs a DNS service for Docker containers.  If you are brand new to using the DevTools VM it's  highly recommended \nto read the  Intro  document (see the docs directory) for a conceptual overview.\nThere is additional documentation in the docs directory that guides you through \na variety of topics, including project and integration environment setup.",
            "title": "DevTools"
        },
        {
            "location": "/#installation",
            "text": "As first time setup, install the following:",
            "title": "Installation"
        },
        {
            "location": "/#1-install-virtualbox-50-if-you-arent-on-linux",
            "text": "VirtualBox website",
            "title": "1. Install VirtualBox &gt;= 5.0 if you aren't on Linux"
        },
        {
            "location": "/#2-install-devtools-on-mac-using-homebrew",
            "text": "",
            "title": "2. Install devtools on Mac using Homebrew"
        },
        {
            "location": "/#21-install-the-devtools-tap",
            "text": "brew tap phase2/devtools",
            "title": "2.1 Install the devtools Tap"
        },
        {
            "location": "/#22-install-the-devtools-binary-and-dependencies",
            "text": "brew install phase2/devtools/docker  brew install phase2/devtools/docker-machine  brew install phase2/devtools/docker-compose  brew install devtools",
            "title": "2.2 Install the devtools binary and dependencies"
        },
        {
            "location": "/#23-verify-the-proper-devtools-environment",
            "text": "Run  devtools doctor  to ensure the proper versions of the Docker binaries are available",
            "title": "2.3 Verify the proper devtools environment"
        },
        {
            "location": "/#24-startup-the-container-host",
            "text": "Once everything checks out. run the following command to create   start the new container \nhost (the docker machine). (You will be prompted for your admin password)  devtools start",
            "title": "2.4 Startup the container host"
        },
        {
            "location": "/#25-configure-your-shell-to-use-dev-tools-environment",
            "text": "To configure the shell with the proper Dev Tools environment, run the following command  eval \"$(devtools config)\"  To make this permanent on every terminal you launch, add the following to your \n   .bash_profile, .zshrc or equivalent:  # Support for Dev Tools\n   eval \"$(devtools config)\"",
            "title": "2.5 Configure your shell to use Dev Tools environment"
        },
        {
            "location": "/00_INTRO/",
            "text": "Overview\n\n\nThis is the knowledge base for our Dev Tools project. The goal of Dev Tools is to provide consistent environments \n tooling for our teams to enable delivery of the highest quality work for our clients.\n\n\nBackground / What\u2019s in it for you?\n\n\nDev Tools covers an entire toolbox of solutions to accelerate development and smoothly implement best practices.  We intend to use the Dev Tools umbrella to help us roll these tools and approaches out to the whole company in a more consistent fashion.\n\n\nAcross a wide variety of projects and clients, keeping a development, integration, staging, production and other environments synchronized in terms of server configuration and supporting software versions and tooling has always been a challenge. The multitude of platforms and versions of operating systems and software is only growing. In the past we have used Vagrant in conjunction with a virtual machine (VM) for local development environments, project-based VMs on Dev Cloud for integration environments and tools like Puppet to try and ensure all of those match each other and the deployment environments. \n\n\nIt\u2019s been better than doing it all by hand but it has required building additional expertise above typical system management tools and it\u2019s relatively inefficient to have many project-specific VMs locally. On top of that are the various, sometimes conflicting, versions of development toolchains which the VMs don\u2019t always address.\n\n\nDev Tools is going to address this problem by providing an efficient way to have project/app-specific environments as well as development tooling by using a concept called containerization. This lets us focus on items at a service-and-tools level rather than at a server level and make the details of our hosting implementation more transparent to all technical team members .\n\n\nContainerization\n\n\nIn order to provide lightweight environments and tooling we will be using a technical approach called containers.  Containers attempt to move the unit of environment from server to application. This allows separation of concerns between how an application is configured, how the containers communicate with each other, and where the containers are deployed.  Take an advanced Drupal stack that includes Apache/PHP, MySQL, Memcache, and SOLR. With each component configured to run in its own container, the containers can all run on a single VM for local development and be spread across multiple servers for an optimized production deployment. Let us briefly touch on the technology we will be using and how it conceptually fits together.\n\n\nDocker\n\n\nThe container implementation we use is called Docker which is explained in the intro: \nWhat is Docker\n? This is the way we capture environment units for our application/services and share them with everyone on the team.  Environments are captured as images, similar to a VM, so when anyone runs that image they all start with the exact same set of files. For example, nearly every Phase2 project needs a web server so we have a container image that can be run to provide that service. \n\n\nGlossary\n\n\nFor those that want a bit more detail \n\n\n\n\n\n\nHost Machine\n - Your laptop for the purposes of _devtools_vm. This is where your project's source code, your IDE, browsers, etc run as well as where the virtual machine acting as the Docker Host runs.\n\n\n\n\n\n\nDocker Host\n - This is a Linux-based virtual machine capable of running Docker. One Docker host VM can run multiple containers for multiple projects.\n\n\n\n\n\n\nDocker Image\n - A read only template that can be instantiated as a running container. An image might supply a service as small as a build tool or as large as a database and/or web server. Images are generally single purpose in nature and are then linked together to build more complex capabilities.\n\n\n\n\n\n\nContainer\n - A runtime instance of a Docker Image. This is what contains a service like Apache or MySQL. Several containers can run on a single Docker Host and they can be linked so that they they know how to communicate with each other.\n\n\n\n\n\n\nDocker Machine\n\n\nDocker Machine creates the Docker Host virtual machine in which containers run.  This provides the functionality that we previously used Vagrant to accomplish. Under the covers docker-machine starts up a virtual machine running a tiny version of Linux.  On Macs this tiny version of Linux is called boot2docker.  All of the Docker commands to build and run containers will actually be executed on the boot2docker virtual machine. On your laptop you will have a single boot2docker virtual machine (running in VirtualBox or VMWare Fusion) and it will host one or more Docker containers. Each project will likely have multiple Docker containers running (web server, database, memcache, search, etc.).\n\n\nDocker Compose\n\n\nDocker Compose is used to manage and coordinate the containers that need to run for a project in an easy to use YAML file. Each project will have a docker-compose file for each Docker environment that the project supports.  For example, the default docker-compose.yml would be used to start containers for local development, and alternate Docker compose files could be included for starting containers for the integration/stage and other environments.",
            "title": "Introduction"
        },
        {
            "location": "/00_INTRO/#overview",
            "text": "This is the knowledge base for our Dev Tools project. The goal of Dev Tools is to provide consistent environments   tooling for our teams to enable delivery of the highest quality work for our clients.",
            "title": "Overview"
        },
        {
            "location": "/00_INTRO/#background-whats-in-it-for-you",
            "text": "Dev Tools covers an entire toolbox of solutions to accelerate development and smoothly implement best practices.  We intend to use the Dev Tools umbrella to help us roll these tools and approaches out to the whole company in a more consistent fashion.  Across a wide variety of projects and clients, keeping a development, integration, staging, production and other environments synchronized in terms of server configuration and supporting software versions and tooling has always been a challenge. The multitude of platforms and versions of operating systems and software is only growing. In the past we have used Vagrant in conjunction with a virtual machine (VM) for local development environments, project-based VMs on Dev Cloud for integration environments and tools like Puppet to try and ensure all of those match each other and the deployment environments.   It\u2019s been better than doing it all by hand but it has required building additional expertise above typical system management tools and it\u2019s relatively inefficient to have many project-specific VMs locally. On top of that are the various, sometimes conflicting, versions of development toolchains which the VMs don\u2019t always address.  Dev Tools is going to address this problem by providing an efficient way to have project/app-specific environments as well as development tooling by using a concept called containerization. This lets us focus on items at a service-and-tools level rather than at a server level and make the details of our hosting implementation more transparent to all technical team members .",
            "title": "Background / What\u2019s in it for you?"
        },
        {
            "location": "/00_INTRO/#containerization",
            "text": "In order to provide lightweight environments and tooling we will be using a technical approach called containers.  Containers attempt to move the unit of environment from server to application. This allows separation of concerns between how an application is configured, how the containers communicate with each other, and where the containers are deployed.  Take an advanced Drupal stack that includes Apache/PHP, MySQL, Memcache, and SOLR. With each component configured to run in its own container, the containers can all run on a single VM for local development and be spread across multiple servers for an optimized production deployment. Let us briefly touch on the technology we will be using and how it conceptually fits together.",
            "title": "Containerization"
        },
        {
            "location": "/00_INTRO/#docker",
            "text": "The container implementation we use is called Docker which is explained in the intro:  What is Docker ? This is the way we capture environment units for our application/services and share them with everyone on the team.  Environments are captured as images, similar to a VM, so when anyone runs that image they all start with the exact same set of files. For example, nearly every Phase2 project needs a web server so we have a container image that can be run to provide that service.",
            "title": "Docker"
        },
        {
            "location": "/00_INTRO/#glossary",
            "text": "For those that want a bit more detail     Host Machine  - Your laptop for the purposes of _devtools_vm. This is where your project's source code, your IDE, browsers, etc run as well as where the virtual machine acting as the Docker Host runs.    Docker Host  - This is a Linux-based virtual machine capable of running Docker. One Docker host VM can run multiple containers for multiple projects.    Docker Image  - A read only template that can be instantiated as a running container. An image might supply a service as small as a build tool or as large as a database and/or web server. Images are generally single purpose in nature and are then linked together to build more complex capabilities.    Container  - A runtime instance of a Docker Image. This is what contains a service like Apache or MySQL. Several containers can run on a single Docker Host and they can be linked so that they they know how to communicate with each other.",
            "title": "Glossary"
        },
        {
            "location": "/00_INTRO/#docker-machine",
            "text": "Docker Machine creates the Docker Host virtual machine in which containers run.  This provides the functionality that we previously used Vagrant to accomplish. Under the covers docker-machine starts up a virtual machine running a tiny version of Linux.  On Macs this tiny version of Linux is called boot2docker.  All of the Docker commands to build and run containers will actually be executed on the boot2docker virtual machine. On your laptop you will have a single boot2docker virtual machine (running in VirtualBox or VMWare Fusion) and it will host one or more Docker containers. Each project will likely have multiple Docker containers running (web server, database, memcache, search, etc.).",
            "title": "Docker Machine"
        },
        {
            "location": "/00_INTRO/#docker-compose",
            "text": "Docker Compose is used to manage and coordinate the containers that need to run for a project in an easy to use YAML file. Each project will have a docker-compose file for each Docker environment that the project supports.  For example, the default docker-compose.yml would be used to start containers for local development, and alternate Docker compose files could be included for starting containers for the integration/stage and other environments.",
            "title": "Docker Compose"
        },
        {
            "location": "/10_TOOLCHAIN/",
            "text": "Phase2 Tooling\n\n\nNow that we\u2019ve covered the technology we\u2019re using, here is a run down on the tooling that ties it all together and that you\u2019ll download to get started.\n\n\n_devtools_vm\n\n\nThis is a Phase2 project that glues all of the hosting aspects of these tools together into an easy to use unit. You can find _devtools_vm in our BitBucket repository \nhere\n.  There are 2 basic services that _devtools_vm provides: \n\n\nManage the virtual machine for running containers\n\n\nThe devtools binary will manage the creation/configuration/upgrade/start/stop of the boot2docker virtual machine (a.k.a Docker Host) via docker-machine.  It ensures that the docker-machine virtual machine is the right version, is named correctly and configured to run efficiently within Virtualbox, VMWare Fusion or xhyve.\n\n\nNice DNS names and routing for running containers\n\n\nOnce there is a safe environment to run our containers we need a way to route traffic to them and provide easy to use/remember domain names to make accessing these services simple. Domain names for containers are set in the docker-compose yaml files using the \nDNSDOCK_NAME\n and \nDNSDOCK_IMAGE\n environment variables.\n\n\nWe use a pair of services: \ndnsdock\n and \ndnsmasq\n running as containers within the Docker Host. \ndnsmasq\n listens on 172.17.42.1:53 and it is currently configured to send all queries for *.p2devcloud.com to our internal 10.10.7.2 resolver and the rest will be sent to the \ndnsdock\n service, which listens on 172.17.42.1:535353. The \ndnsdock\n container resolves the .vm domain names to the IP addresses of the containers.\n\n\nThe practical effect of this is that you must be on the Phase2 VPN or on the private Wi-Fi of a Phase2 office to get access to *.p2devcloud.com domains, but once you are either on the VPN or in an office, you will bypass the Dev Cloud public proxy.\n\n\nInternal container names will look like \nweb.openatrium.vm\n.  All devtools containers will carry the \n.vm\n extension for name resolution. There is additional information about these variables in \n20_PROJECT_SETUP.md\n.\n\n\np2docker\n\n\nThe name p2docker refers to a collection of Docker Images that we maintain to provide consistent and easily configurable services to our projects.  If you are attempting to put together a docker-compose configuration file, you should look at the container images provided by p2docker as a source of images to use. You can also look at Docker Hub for other community containers of interest as well. The \np2docker repo\n contains the source for common Phase2 Docker Images that are hosted in the \nPhase2 Docker Hub\n account.\n\n\nDocker Hub\n\n\nDocker Hub is where container images are stored and retrieved when your local machine does not already have a copy of the requested container image.  Docker Hub can be thought of like GitHub or BitBucket and Docker Hub images can be thought of as git repositories. We can make new versions of the images and they can be pushed and pulled to the Docker Hub service.\n\n\nPutting it all together\n\n\nThere are some important pieces to remember about how containerization is similar and different to concepts you may be familiar with.\n\n\nVagrant\n\n\nVagrant is being replaced by the combination of Docker Compose and Docker Machine.\n\n\nVirtual Machines\n\n\nFor local development, you should no longer worry about Virtual Machines as they are replaced by a single Docker Machine instance. In environments where containers are meant to run on a cluster of hosts, orchestration tools like \nKubernetes\n or \nNomad\n may be used to distribute and coordinate containers across multiple Docker Hosts.\n\n\nPuppet\n\n\nGoodbye, for local environments at least. Docker uses a conceptually different approach to configuration management than Puppet. It\u2019s perfectly possible to use Puppet from within a Dockerfile to get a container image prepared though typically simple shell commands are preferred.  The idea is that systems like Puppet are no longer needed to manage upgrades across working servers/containers. When there are updates needed you will update the image, pull down the new version of the image to your server and then stop the containers running the old version of the image and start containers based on the new version of the image.\n\n\nIf a container wants to offer configurable options it will document how to control it within the README or via Environment variables in the Dockerfile itself.  See our Apache / PHP \nDockerfile\n for an example. In this Docker Image, passing environment variables can override the PHP memory limit. Those variables can either be passed on the command line when executing a \ndocker run\n command directly, or in the \nenvironment\n section of a docker-compose file.\n\n\nProject Code\n\n\nThe filesystem within containers is \nephemeral!\n Any changes made there \ndo not persist\n if the container is restarted. (See \"Persistent Data Volume\" below for a storage area that is preserved.)\n\n\nIn typical Docker images the code is built \ndirectly into the container\n at the \n/code\n path. This is a great mechanism that allows the container to be \"self contained\" (pun intended), immutable and not need any checkout/file system. You also know when you run a container exactly what code is in there because you generally don\u2019t change the code unless you rebuild the image.  \n\n\nFor development purposes, however, this is problematic because it is burdensome to rebuild an image for each code change. To solve this, we mount project code from the Host Machine into the running container, effectively overriding the files built directly into the image. The running container is then using the local project file system for the overridden paths rather than the file system built into the container.  This allows a developer to use an IDE and edit code directly on the local file system of the Host Machine, but execute that code within the environment of the running container.\n\n\nNOTE ON PROJECT CODE LOCATION\n\n\nYour project code \nmust\n be located somewhere within your home directory (\n/Users\n for Macs) on your local machine. This is because VirtualBox and VMWare shares your home directory into the Docker Host VM, and only files on the Docker Host VM can be referenced in volume shares from a Docker compose file. Within the volume portions of a docker-compose file there is also a bit of translation that happens for relative paths. If you ask for the ./html directory from your local machine to appear as \n/var/www/html\n using a line like: \n./html:/var/www/html\n that is translated to a full path based on the location of the docker-compose file. If that doesn\u2019t work out to something under your home directory the sharing of files from your Host Machine isn\u2019t going to work. If you specify a full path, it also needs to be from your home directory.\n\n\nNOTE ON WATCHES\n\n\nCurrently, code within a container cannot be \"watched\" within the container as filesystem notify events do not propagate into the container.  Therefore when you change code you may need to restart the services that execute that code to see the changes. (This can be a problem for services like node or sass, while services like PHP work fine).  In order for a \u201cwatch\u201d to run effectively it should be run on your Host Machine and can initiate commands into your running container via a \ndocker exec\n if needed via \ndocker exec -it \ncontainer_name\n \nyour command(s)\n\n\nPersistent Data Volume\n\n\nDev Tools maintains a data volume on the Docker Host that is mounted at \n/data\n into every container. This volume is persistent so long as you do not perform a \ndevtools remove\n operation. This ensures that file access on the VM is done natively for things like MongoDB and MySQL. If you configure a container to write to this area, you should use a project and container based namespace to prevent conflicts as this is a shared resource. For example, you may want to use a namespacing method like \n/data/${DNSDOCK_IMAGE}/${DNSDOCK_NAME}\n as a safe location to write data.\n\n\nAny code from your local directories is directly shared in to VirtualBox via NFS. This means that even if you destroy and re-create your Docker Host, your code will be safe since it lives on the Host Machine.\n\n\nNOTE ON FILE CHANGES WITHIN A CONTAINER\n\n\nAny files that are generated or changed within a running container that you want to persist after the container is stopped \nshould be put onto a volume that is mounted into the container from the local machine\n.  A Docker container represents immutable infrastructure, the files on the image are able to be changed at runtime but typically do not persist. When the container is stopped and run again it \"boots\" the files that were built into the original image. The volume at \n/data\n as mentioned above is a persistent volume that you can use.",
            "title": "Tool Chain"
        },
        {
            "location": "/10_TOOLCHAIN/#phase2-tooling",
            "text": "Now that we\u2019ve covered the technology we\u2019re using, here is a run down on the tooling that ties it all together and that you\u2019ll download to get started.",
            "title": "Phase2 Tooling"
        },
        {
            "location": "/10_TOOLCHAIN/#_devtools_vm",
            "text": "This is a Phase2 project that glues all of the hosting aspects of these tools together into an easy to use unit. You can find _devtools_vm in our BitBucket repository  here .  There are 2 basic services that _devtools_vm provides:",
            "title": "_devtools_vm"
        },
        {
            "location": "/10_TOOLCHAIN/#manage-the-virtual-machine-for-running-containers",
            "text": "The devtools binary will manage the creation/configuration/upgrade/start/stop of the boot2docker virtual machine (a.k.a Docker Host) via docker-machine.  It ensures that the docker-machine virtual machine is the right version, is named correctly and configured to run efficiently within Virtualbox, VMWare Fusion or xhyve.",
            "title": "Manage the virtual machine for running containers"
        },
        {
            "location": "/10_TOOLCHAIN/#nice-dns-names-and-routing-for-running-containers",
            "text": "Once there is a safe environment to run our containers we need a way to route traffic to them and provide easy to use/remember domain names to make accessing these services simple. Domain names for containers are set in the docker-compose yaml files using the  DNSDOCK_NAME  and  DNSDOCK_IMAGE  environment variables.  We use a pair of services:  dnsdock  and  dnsmasq  running as containers within the Docker Host.  dnsmasq  listens on 172.17.42.1:53 and it is currently configured to send all queries for *.p2devcloud.com to our internal 10.10.7.2 resolver and the rest will be sent to the  dnsdock  service, which listens on 172.17.42.1:535353. The  dnsdock  container resolves the .vm domain names to the IP addresses of the containers.  The practical effect of this is that you must be on the Phase2 VPN or on the private Wi-Fi of a Phase2 office to get access to *.p2devcloud.com domains, but once you are either on the VPN or in an office, you will bypass the Dev Cloud public proxy.  Internal container names will look like  web.openatrium.vm .  All devtools containers will carry the  .vm  extension for name resolution. There is additional information about these variables in  20_PROJECT_SETUP.md .",
            "title": "Nice DNS names and routing for running containers"
        },
        {
            "location": "/10_TOOLCHAIN/#p2docker",
            "text": "The name p2docker refers to a collection of Docker Images that we maintain to provide consistent and easily configurable services to our projects.  If you are attempting to put together a docker-compose configuration file, you should look at the container images provided by p2docker as a source of images to use. You can also look at Docker Hub for other community containers of interest as well. The  p2docker repo  contains the source for common Phase2 Docker Images that are hosted in the  Phase2 Docker Hub  account.",
            "title": "p2docker"
        },
        {
            "location": "/10_TOOLCHAIN/#docker-hub",
            "text": "Docker Hub is where container images are stored and retrieved when your local machine does not already have a copy of the requested container image.  Docker Hub can be thought of like GitHub or BitBucket and Docker Hub images can be thought of as git repositories. We can make new versions of the images and they can be pushed and pulled to the Docker Hub service.",
            "title": "Docker Hub"
        },
        {
            "location": "/10_TOOLCHAIN/#putting-it-all-together",
            "text": "There are some important pieces to remember about how containerization is similar and different to concepts you may be familiar with.",
            "title": "Putting it all together"
        },
        {
            "location": "/10_TOOLCHAIN/#vagrant",
            "text": "Vagrant is being replaced by the combination of Docker Compose and Docker Machine.",
            "title": "Vagrant"
        },
        {
            "location": "/10_TOOLCHAIN/#virtual-machines",
            "text": "For local development, you should no longer worry about Virtual Machines as they are replaced by a single Docker Machine instance. In environments where containers are meant to run on a cluster of hosts, orchestration tools like  Kubernetes  or  Nomad  may be used to distribute and coordinate containers across multiple Docker Hosts.",
            "title": "Virtual Machines"
        },
        {
            "location": "/10_TOOLCHAIN/#puppet",
            "text": "Goodbye, for local environments at least. Docker uses a conceptually different approach to configuration management than Puppet. It\u2019s perfectly possible to use Puppet from within a Dockerfile to get a container image prepared though typically simple shell commands are preferred.  The idea is that systems like Puppet are no longer needed to manage upgrades across working servers/containers. When there are updates needed you will update the image, pull down the new version of the image to your server and then stop the containers running the old version of the image and start containers based on the new version of the image.  If a container wants to offer configurable options it will document how to control it within the README or via Environment variables in the Dockerfile itself.  See our Apache / PHP  Dockerfile  for an example. In this Docker Image, passing environment variables can override the PHP memory limit. Those variables can either be passed on the command line when executing a  docker run  command directly, or in the  environment  section of a docker-compose file.",
            "title": "Puppet"
        },
        {
            "location": "/10_TOOLCHAIN/#project-code",
            "text": "The filesystem within containers is  ephemeral!  Any changes made there  do not persist  if the container is restarted. (See \"Persistent Data Volume\" below for a storage area that is preserved.)  In typical Docker images the code is built  directly into the container  at the  /code  path. This is a great mechanism that allows the container to be \"self contained\" (pun intended), immutable and not need any checkout/file system. You also know when you run a container exactly what code is in there because you generally don\u2019t change the code unless you rebuild the image.    For development purposes, however, this is problematic because it is burdensome to rebuild an image for each code change. To solve this, we mount project code from the Host Machine into the running container, effectively overriding the files built directly into the image. The running container is then using the local project file system for the overridden paths rather than the file system built into the container.  This allows a developer to use an IDE and edit code directly on the local file system of the Host Machine, but execute that code within the environment of the running container.",
            "title": "Project Code"
        },
        {
            "location": "/10_TOOLCHAIN/#note-on-project-code-location",
            "text": "Your project code  must  be located somewhere within your home directory ( /Users  for Macs) on your local machine. This is because VirtualBox and VMWare shares your home directory into the Docker Host VM, and only files on the Docker Host VM can be referenced in volume shares from a Docker compose file. Within the volume portions of a docker-compose file there is also a bit of translation that happens for relative paths. If you ask for the ./html directory from your local machine to appear as  /var/www/html  using a line like:  ./html:/var/www/html  that is translated to a full path based on the location of the docker-compose file. If that doesn\u2019t work out to something under your home directory the sharing of files from your Host Machine isn\u2019t going to work. If you specify a full path, it also needs to be from your home directory.",
            "title": "NOTE ON PROJECT CODE LOCATION"
        },
        {
            "location": "/10_TOOLCHAIN/#note-on-watches",
            "text": "Currently, code within a container cannot be \"watched\" within the container as filesystem notify events do not propagate into the container.  Therefore when you change code you may need to restart the services that execute that code to see the changes. (This can be a problem for services like node or sass, while services like PHP work fine).  In order for a \u201cwatch\u201d to run effectively it should be run on your Host Machine and can initiate commands into your running container via a  docker exec  if needed via  docker exec -it  container_name   your command(s)",
            "title": "NOTE ON WATCHES"
        },
        {
            "location": "/10_TOOLCHAIN/#persistent-data-volume",
            "text": "Dev Tools maintains a data volume on the Docker Host that is mounted at  /data  into every container. This volume is persistent so long as you do not perform a  devtools remove  operation. This ensures that file access on the VM is done natively for things like MongoDB and MySQL. If you configure a container to write to this area, you should use a project and container based namespace to prevent conflicts as this is a shared resource. For example, you may want to use a namespacing method like  /data/${DNSDOCK_IMAGE}/${DNSDOCK_NAME}  as a safe location to write data.  Any code from your local directories is directly shared in to VirtualBox via NFS. This means that even if you destroy and re-create your Docker Host, your code will be safe since it lives on the Host Machine.",
            "title": "Persistent Data Volume"
        },
        {
            "location": "/10_TOOLCHAIN/#note-on-file-changes-within-a-container",
            "text": "Any files that are generated or changed within a running container that you want to persist after the container is stopped  should be put onto a volume that is mounted into the container from the local machine .  A Docker container represents immutable infrastructure, the files on the image are able to be changed at runtime but typically do not persist. When the container is stopped and run again it \"boots\" the files that were built into the original image. The volume at  /data  as mentioned above is a persistent volume that you can use.",
            "title": "NOTE ON FILE CHANGES WITHIN A CONTAINER"
        },
        {
            "location": "/20_PROJECT_SETUP/",
            "text": "How to get started\n\n\n\n\n\n\nInstall DevTools via Homebrew. First, tap the Homebrew repository \nbrew tap phase2/devtools\n, then install DevTools \nbrew install devtools\n.  If needed you should also install all required docker tools form the DevTools Tap. \nbrew install phase2/devtools/docker phase2/devtools/docker-machine phase2/devtools/docker-compose\n \n\n\n\n\n\n\nFollow the instructions in the \nDevTools Index\n to get your environment configured. All of the following sample commands will assume you\u2019ve installed things as suggested in the README.\n\n\n\n\n\n\nGet ready to run containers\n\n\nWhen you want to start the Docker containers for a project, do the following:\n\n\n\n\n\n\nEnsure the Docker Host VM is active by running: \ndevtools start\n\n\n\n\n\n\nEnsure all terminals you intend to use have \neval \"$(devtools config)\"\n run in them. It is best to put this configuration in your .bashrc/.zshrc, see the main \nDevTools Index\n for more details\n\n\n\n\n\n\nIn the project directory, start the containers with: \ndocker-compose up\n\n\n\n\n\n\nNote: The docker-compose command runs in the foreground as long as the Docker containers are running. \n(It is not hung if there is no output after \"Attaching \n...\")\n\n\n\n\n\n\nLogs from the running containers will stream to the console and be prefixed by their compose names plus an integer (i.e. web_1, db_1, etc)\n\n\n\n\n\n\n\n\n\n\nStart another terminal if you need to run other commands (such as the build container cli)\n\n\n\n\n\n\nStopping the containers for your project\n\n\nYou may want to recoup the resources used by your projects containers and the docker host for other things if you are done with development for a while.  You can stop the containers for a single project only or all containers and the docker host depending on what you are finished using.\n\n\n\n\n\n\nIf you only want to stop the containers for your project, in the project directory, run \ndocker-compose stop\n or press Ctrl-C to stop a docker-compose process running in the foreground and then run \ndocker-compose stop\n to ensure the project containers have stopped.\n\n\n\n\n\n\nIf you want to shut down the docker host as well as any containers, run \ndevtools stop\n\n\n\n\n\n\nCleaning Up\n\n\nFrom time to time you'll want to clean up stopped containers. You'll also want to take special care when finishing a project to release all the resources used by it.\n\n\nFor periodic cleanup of all stopped containers, run the following script while your docker host is running: \ndevtools prune\n\n\nIf you only want to clean up project specific stopped containers, you can run: \ndocker-compose rm\n from your project directory.\n\n\nWhen you are finished with a project, if you used any persistent data storage you'll want to run a command to clean it up. The exact directory to request removal from will depend on your project (see suggested namespace guidelines in the TOOLCHAIN document): \ndocker ssh dev sudo rm -rf /data/PROJECT_DIR\n\n\nSetting up a project\n\n\nTo set up a project rather than just work on one, you'll need to understand how to create a docker-compose file, how to configure DNS for your project's containers and how to configure persistent storage if your project needs it.\n\n\nThe docker-compose file\n\n\nLearn more about Docker Compose here: \nhttps://docs.docker.com/compose/\n\n\nTake a look at \nexample/drupal8\n for an example docker-compose file\n\n\nDNS Names for your containers\n\n\nWithin the environment section of the docker-compose file you can specify variables that will control the DNS name of your containers  \n\n\n\n\n\n\nDNSDOCK_NAME - This is the type of container. Usually something like web, db, cache, etc.\n\n\n\n\n\n\nDNSDOCK_IMAGE - This is generally your project name (e.g fieldnotes, openatrium, etc.)\n\n\n\n\n\n\nAll DNS names will end in \n.vm\n\n\n\n\n\n\nThose items combine so that you can refer to services running in the containers using domain names of the form \nDNSDOCK_NAME.DNSDOCK_IMAGE.vm\n (e.g web.phase2.vm)\n\n\n\n\n\n\nIf you need multiple domains mapped to a container you can use the DNSDOCK_ALIAS setting. It takes a full domain name and can take a comma separated list (e.g. portland.phase2.vm,alexandria.phase2.vm)\n\n\n\n\n\n\nDNS for Dev Cloud\n\n\nDev Tools will resolve private Dev Cloud domain names if you are connected to the VPN.  You need to have access to 10.10.7.2 to resolve Dev Cloud domain names.\n\n\nWorking with Volumes\n\n\nVolumes are a way that you can map directories or individual files into a running container.  This is useful to provide code to run for a generic container, or directories to store data that persist longer than the life of the container, or to even override directories and/or files that exist in an image.  Lets look at a few of those examples.\n\n\nProvide code for a generic container to run\n\n\nThe default phase2/apache24php55 is a Web/PHP container that provides only an index file in \n/var/www/html\n that prints out phpinfo().  This is obviously not very useful for an application, so we can provide the an entire Drupal site to run and we do that by mapping our site into the default docroot like this:\n\n\n./build/html:/var/www/html\n\n\nThis takes the Drupal site we have in our local project directory of \nbuild/html\n and it overrides the default content of the images \n/var/www/html\n directory.\n\n\nPersist data longer than the life of the container\n\n\nWhen using a database like mysql the container will store the database files in \n/var/lib/mysql\n and by default that directory will be reset every time the container restarts. This means each time you restart your container you\u2019d need to reinstall your application and database, which can make life difficult. So in order to persist mysql data for longer than the current run of the container we will map a directory from the Docker Host into the container and override the default \n/var/lib/mysql\n directory. The configuration will look something like this.\n\n\n/data/drupal/mysql:/var/lib/mysql\n\n\nNow when your database container creates files in \n/var/lib/mysql\n they are actually saved in the \n/data/drupal/mysql\n directory on the Docker Host. Be sure to namespace your directories inside /data so that separate projects don't conflict with each other. You'll also want to ensure you clean up when you are done with your project to keep from using up all of your disk space. See the cleaning up section for more info.\n\n\nOverride directories and files that exist in an image\n\n\nGenerally the configuration shipped with a Docker Image is meant to be the production configuration. Often, that configuration is not suitable for development and we need to override configurations.  With volumes we have shown earlier how you can override directories, but you can also override individual files too.\n\n\n./config/dev/httpd/httpd.conf:/etc/httpd/httpd.conf\n\n\nThis takes a local \nhttpd.conf\n file from our project and overrides the \n/etc/httpd/httpd.conf\n files that ships with the container. \n\n\nResetting a Docker Container\n\n\nIf you change the docker-compose.yml configuration, for example to add/remove/change an environment variable, or you pull down a new version of a docker image, you can reset the Docker container simply by restarting.\n\n\nHowever, if you wind up changing volume definitions in your docker-compose file you will need to remove your container before it will recognize those changes on a restart.\n\n\nIn the project directory, run: \ndocker-compose rm\n\n\nThis command will remove all stopped containers defined in the docker-compose.yml.  You could also  remove an individual container by running: \ndocker-compose rm \nname in yml\n\n\nAfter removing the container, it will revert to the original state from the Docker image, removing any packages installed or files modified that are not included in a volume mount. Start your containers again with \ndocker-compose up\n and you should have your new volume mounts.\n\n\nCreating a Dockerfile\n\n\nThe Dockerfile you create for your project should represent the application as it will need to run in production. There are many reasons to have a Dockerfile to create an image, but if you don\u2019t intend on running containers in production you can likely skip this part. \n\n\nThe most minimal project container will need to have your code in it.\n\n\n\n\n\n\nStart by determining which Docker image you will need to extend. \n\n\n\n\n\n\nFor the general Drupal use cases you will base your project Dockerfile on apache24php55.\n\n\n\n\nFROM phase2/apache24php55\n\n\n\n\n\n\n\n\nThen copy your code into the correct place for the container. For a PHP/Drupal project, copy the materialized site code into the container docroot\n\n\n\n\nCOPY ./html /var/www/html/\n\n\n\n\n\n\n\n\nBuild the Dockerfile into an image\n\n\n\n\ndocker build -t \nsome-name\n .\n\n\n\n\n\n\n\n\nOn successful build, test the image by running\n\n\n\n\ndocker run -t \nsome-name\n\n\n\n\n\n\n\n\nHere is the full (simple) Dockerfile\n\n\nFROM phase2/apache24php55\n\n# Copy in the site\nCOPY ./html /var/www/html/",
            "title": "Project Setup"
        },
        {
            "location": "/20_PROJECT_SETUP/#how-to-get-started",
            "text": "Install DevTools via Homebrew. First, tap the Homebrew repository  brew tap phase2/devtools , then install DevTools  brew install devtools .  If needed you should also install all required docker tools form the DevTools Tap.  brew install phase2/devtools/docker phase2/devtools/docker-machine phase2/devtools/docker-compose      Follow the instructions in the  DevTools Index  to get your environment configured. All of the following sample commands will assume you\u2019ve installed things as suggested in the README.",
            "title": "How to get started"
        },
        {
            "location": "/20_PROJECT_SETUP/#get-ready-to-run-containers",
            "text": "When you want to start the Docker containers for a project, do the following:    Ensure the Docker Host VM is active by running:  devtools start    Ensure all terminals you intend to use have  eval \"$(devtools config)\"  run in them. It is best to put this configuration in your .bashrc/.zshrc, see the main  DevTools Index  for more details    In the project directory, start the containers with:  docker-compose up    Note: The docker-compose command runs in the foreground as long as the Docker containers are running.  (It is not hung if there is no output after \"Attaching  ...\")    Logs from the running containers will stream to the console and be prefixed by their compose names plus an integer (i.e. web_1, db_1, etc)      Start another terminal if you need to run other commands (such as the build container cli)",
            "title": "Get ready to run containers"
        },
        {
            "location": "/20_PROJECT_SETUP/#stopping-the-containers-for-your-project",
            "text": "You may want to recoup the resources used by your projects containers and the docker host for other things if you are done with development for a while.  You can stop the containers for a single project only or all containers and the docker host depending on what you are finished using.    If you only want to stop the containers for your project, in the project directory, run  docker-compose stop  or press Ctrl-C to stop a docker-compose process running in the foreground and then run  docker-compose stop  to ensure the project containers have stopped.    If you want to shut down the docker host as well as any containers, run  devtools stop",
            "title": "Stopping the containers for your project"
        },
        {
            "location": "/20_PROJECT_SETUP/#cleaning-up",
            "text": "From time to time you'll want to clean up stopped containers. You'll also want to take special care when finishing a project to release all the resources used by it.  For periodic cleanup of all stopped containers, run the following script while your docker host is running:  devtools prune  If you only want to clean up project specific stopped containers, you can run:  docker-compose rm  from your project directory.  When you are finished with a project, if you used any persistent data storage you'll want to run a command to clean it up. The exact directory to request removal from will depend on your project (see suggested namespace guidelines in the TOOLCHAIN document):  docker ssh dev sudo rm -rf /data/PROJECT_DIR",
            "title": "Cleaning Up"
        },
        {
            "location": "/20_PROJECT_SETUP/#setting-up-a-project",
            "text": "To set up a project rather than just work on one, you'll need to understand how to create a docker-compose file, how to configure DNS for your project's containers and how to configure persistent storage if your project needs it.",
            "title": "Setting up a project"
        },
        {
            "location": "/20_PROJECT_SETUP/#the-docker-compose-file",
            "text": "Learn more about Docker Compose here:  https://docs.docker.com/compose/  Take a look at  example/drupal8  for an example docker-compose file",
            "title": "The docker-compose file"
        },
        {
            "location": "/20_PROJECT_SETUP/#dns-names-for-your-containers",
            "text": "Within the environment section of the docker-compose file you can specify variables that will control the DNS name of your containers      DNSDOCK_NAME - This is the type of container. Usually something like web, db, cache, etc.    DNSDOCK_IMAGE - This is generally your project name (e.g fieldnotes, openatrium, etc.)    All DNS names will end in  .vm    Those items combine so that you can refer to services running in the containers using domain names of the form  DNSDOCK_NAME.DNSDOCK_IMAGE.vm  (e.g web.phase2.vm)    If you need multiple domains mapped to a container you can use the DNSDOCK_ALIAS setting. It takes a full domain name and can take a comma separated list (e.g. portland.phase2.vm,alexandria.phase2.vm)",
            "title": "DNS Names for your containers"
        },
        {
            "location": "/20_PROJECT_SETUP/#dns-for-dev-cloud",
            "text": "Dev Tools will resolve private Dev Cloud domain names if you are connected to the VPN.  You need to have access to 10.10.7.2 to resolve Dev Cloud domain names.",
            "title": "DNS for Dev Cloud"
        },
        {
            "location": "/20_PROJECT_SETUP/#working-with-volumes",
            "text": "Volumes are a way that you can map directories or individual files into a running container.  This is useful to provide code to run for a generic container, or directories to store data that persist longer than the life of the container, or to even override directories and/or files that exist in an image.  Lets look at a few of those examples.  Provide code for a generic container to run  The default phase2/apache24php55 is a Web/PHP container that provides only an index file in  /var/www/html  that prints out phpinfo().  This is obviously not very useful for an application, so we can provide the an entire Drupal site to run and we do that by mapping our site into the default docroot like this:  ./build/html:/var/www/html  This takes the Drupal site we have in our local project directory of  build/html  and it overrides the default content of the images  /var/www/html  directory.  Persist data longer than the life of the container  When using a database like mysql the container will store the database files in  /var/lib/mysql  and by default that directory will be reset every time the container restarts. This means each time you restart your container you\u2019d need to reinstall your application and database, which can make life difficult. So in order to persist mysql data for longer than the current run of the container we will map a directory from the Docker Host into the container and override the default  /var/lib/mysql  directory. The configuration will look something like this.  /data/drupal/mysql:/var/lib/mysql  Now when your database container creates files in  /var/lib/mysql  they are actually saved in the  /data/drupal/mysql  directory on the Docker Host. Be sure to namespace your directories inside /data so that separate projects don't conflict with each other. You'll also want to ensure you clean up when you are done with your project to keep from using up all of your disk space. See the cleaning up section for more info.  Override directories and files that exist in an image  Generally the configuration shipped with a Docker Image is meant to be the production configuration. Often, that configuration is not suitable for development and we need to override configurations.  With volumes we have shown earlier how you can override directories, but you can also override individual files too.  ./config/dev/httpd/httpd.conf:/etc/httpd/httpd.conf  This takes a local  httpd.conf  file from our project and overrides the  /etc/httpd/httpd.conf  files that ships with the container.",
            "title": "Working with Volumes"
        },
        {
            "location": "/20_PROJECT_SETUP/#resetting-a-docker-container",
            "text": "If you change the docker-compose.yml configuration, for example to add/remove/change an environment variable, or you pull down a new version of a docker image, you can reset the Docker container simply by restarting.  However, if you wind up changing volume definitions in your docker-compose file you will need to remove your container before it will recognize those changes on a restart.  In the project directory, run:  docker-compose rm  This command will remove all stopped containers defined in the docker-compose.yml.  You could also  remove an individual container by running:  docker-compose rm  name in yml  After removing the container, it will revert to the original state from the Docker image, removing any packages installed or files modified that are not included in a volume mount. Start your containers again with  docker-compose up  and you should have your new volume mounts.",
            "title": "Resetting a Docker Container"
        },
        {
            "location": "/20_PROJECT_SETUP/#creating-a-dockerfile",
            "text": "The Dockerfile you create for your project should represent the application as it will need to run in production. There are many reasons to have a Dockerfile to create an image, but if you don\u2019t intend on running containers in production you can likely skip this part.   The most minimal project container will need to have your code in it.    Start by determining which Docker image you will need to extend.     For the general Drupal use cases you will base your project Dockerfile on apache24php55.   FROM phase2/apache24php55     Then copy your code into the correct place for the container. For a PHP/Drupal project, copy the materialized site code into the container docroot   COPY ./html /var/www/html/     Build the Dockerfile into an image   docker build -t  some-name  .     On successful build, test the image by running   docker run -t  some-name     Here is the full (simple) Dockerfile  FROM phase2/apache24php55\n\n# Copy in the site\nCOPY ./html /var/www/html/",
            "title": "Creating a Dockerfile"
        },
        {
            "location": "/30_PROJECT_USAGE/",
            "text": "Project Usage\n\n\nWatches\n\n\nOften we need \nwatches\n running inside of our containers. This could be for webpack, grunt, nodemon, etc.  One of the main challenges with the NFS mounts used in DevTools is that they do not forward filesystem notifications across the NFS mount and into containers, so we need to facilitate that.\n\n\nThere is a command \ndevtools watch [options] \npath\n. Running this on your host will watch for changes and rsync notifications to files and directories under \npath\n into the Docker Machine VM (which will then provide filesystem notifications into the container). These are the options for \ndevtools watch\n\n\n\n\n--machine \nname\n Optional: Specify a machine to send events. It will default to our \ndev\n machine\n\n\n--ignorefile \nfile\n Optional: Specify a file that contains patterns for directories/files to ignore.  Put one entry per line (blank lines and comments are allowed). If not specified it will look for a file named \n.devtools-watch-ignore\n in the working directory and all parent directories.\n\n\n\n\nWhat is the Build container\n\n\nPart of Dev Tools is a \ndevtools-build\n image.  The idea of the build container is that it will have installed nearly all of the tools you'd need to work on a project and the proper versions so that they work well together.  Everything from drush to gem to npm to composer as well as grunt, bower, yeoman, etc. Providing these tools in a container means that you'll never have to worry about having the right tools installed on your laptop or integration environment to get your job done.\n\n\nHow and why to use the build container\n\n\nThe example Drupal project contains a \nbuild.yml\n Docker Compose file that shows a variety of ways to use the container that we will highlight below.\n\n\nGetting a shell for build/tooling operations\n\n\nGetting a shell into a build container to execute any operations is the simplest approach. You simply want to get access to the \ncli\n container we defined in the compose file.  The command \ndocker-compose -f build.yml run cli\n will start an instance of the \nphase2/devtools-build\n image and run a bash shell for you.  From there you are free to use \ndrush\n, \ngrunt\n or whatever your little heart desires.\n\n\nRunning commands, but not from a dedicated shell\n\n\nAnother concept in the Docker world is starting a container to run a single command and allowing the container stop when the command is completed.  This is great if you run commands infrequently, or don't want to have another container constantly running.  Running your commands on containers in this fashion is also well suited for commands that don't generate any files on the filesystem or if they do, they write those files on to volumes mounted into the container.\n\n\nThe \ndrush\n container defined in the example \nbuild.yml\n file is a container designed specifically to run drush in a single working directory taking only the commands as arguments.  This approach allows us to provide a quick and easy mechanism for running any drush command, such as \nsqlc\n, \ncache-rebuild\n, and others, in your Drupal site quick and easily.\n\n\nThere are also other examples of a \ngrunt\n command container similar to \ndrush\n and an even more specific command container around running a single command, \ndrush make\n to build the site from a make/dependency file.\n\n\nForward/Import your SSH Key into the build container to clone private repos\n\n\nCertain containers like jenkins and devtools-build may need your private key. This is supported by importing your private key into the container via a volume mount.  \n\n\nTo get your private key into the build container, volume mount your key into the container at \n/root/.ssh/devtools.key\n and it will be processed accordingly.\n\n\n~/.ssh/id_rsa:/root/.ssh/devtools.key",
            "title": "Project Usage"
        },
        {
            "location": "/30_PROJECT_USAGE/#project-usage",
            "text": "",
            "title": "Project Usage"
        },
        {
            "location": "/30_PROJECT_USAGE/#watches",
            "text": "Often we need  watches  running inside of our containers. This could be for webpack, grunt, nodemon, etc.  One of the main challenges with the NFS mounts used in DevTools is that they do not forward filesystem notifications across the NFS mount and into containers, so we need to facilitate that.  There is a command  devtools watch [options]  path . Running this on your host will watch for changes and rsync notifications to files and directories under  path  into the Docker Machine VM (which will then provide filesystem notifications into the container). These are the options for  devtools watch   --machine  name  Optional: Specify a machine to send events. It will default to our  dev  machine  --ignorefile  file  Optional: Specify a file that contains patterns for directories/files to ignore.  Put one entry per line (blank lines and comments are allowed). If not specified it will look for a file named  .devtools-watch-ignore  in the working directory and all parent directories.",
            "title": "Watches"
        },
        {
            "location": "/30_PROJECT_USAGE/#what-is-the-build-container",
            "text": "Part of Dev Tools is a  devtools-build  image.  The idea of the build container is that it will have installed nearly all of the tools you'd need to work on a project and the proper versions so that they work well together.  Everything from drush to gem to npm to composer as well as grunt, bower, yeoman, etc. Providing these tools in a container means that you'll never have to worry about having the right tools installed on your laptop or integration environment to get your job done.",
            "title": "What is the Build container"
        },
        {
            "location": "/30_PROJECT_USAGE/#how-and-why-to-use-the-build-container",
            "text": "The example Drupal project contains a  build.yml  Docker Compose file that shows a variety of ways to use the container that we will highlight below.",
            "title": "How and why to use the build container"
        },
        {
            "location": "/30_PROJECT_USAGE/#getting-a-shell-for-buildtooling-operations",
            "text": "Getting a shell into a build container to execute any operations is the simplest approach. You simply want to get access to the  cli  container we defined in the compose file.  The command  docker-compose -f build.yml run cli  will start an instance of the  phase2/devtools-build  image and run a bash shell for you.  From there you are free to use  drush ,  grunt  or whatever your little heart desires.",
            "title": "Getting a shell for build/tooling operations"
        },
        {
            "location": "/30_PROJECT_USAGE/#running-commands-but-not-from-a-dedicated-shell",
            "text": "Another concept in the Docker world is starting a container to run a single command and allowing the container stop when the command is completed.  This is great if you run commands infrequently, or don't want to have another container constantly running.  Running your commands on containers in this fashion is also well suited for commands that don't generate any files on the filesystem or if they do, they write those files on to volumes mounted into the container.  The  drush  container defined in the example  build.yml  file is a container designed specifically to run drush in a single working directory taking only the commands as arguments.  This approach allows us to provide a quick and easy mechanism for running any drush command, such as  sqlc ,  cache-rebuild , and others, in your Drupal site quick and easily.  There are also other examples of a  grunt  command container similar to  drush  and an even more specific command container around running a single command,  drush make  to build the site from a make/dependency file.",
            "title": "Running commands, but not from a dedicated shell"
        },
        {
            "location": "/30_PROJECT_USAGE/#forwardimport-your-ssh-key-into-the-build-container-to-clone-private-repos",
            "text": "Certain containers like jenkins and devtools-build may need your private key. This is supported by importing your private key into the container via a volume mount.    To get your private key into the build container, volume mount your key into the container at  /root/.ssh/devtools.key  and it will be processed accordingly.  ~/.ssh/id_rsa:/root/.ssh/devtools.key",
            "title": "Forward/Import your SSH Key into the build container to clone private repos"
        },
        {
            "location": "/40_INTEGRATION/",
            "text": "Integration Environments\n\n\nNow that we use docker-compose we can easily spin up integration environments for our projects.  We are currently using \nci.p2devcloud.com\n and \nci2.p2devcloud.com\n as the integration environment. You can see a list of all integration sites currently running by going to \nhttp://ci.p2devcloud.com\n and \nhttp://ci2.p2devcloud.com\n\n\nHow to get an integration environment going\n\n\nThe good news is that if you have your dev environment running via docker-compose, it should be very straight-forward to pull up an integration environment.  Think of each environment you want to run as having it\u2019s own docker-compose file. See the \nsample integration file\n included in example/drupal8.\n\n\nFor a quick and easy start:\n\n\n\n\n\n\nMake a copy of docker-compose.yml and name it docker-compose.int.yml\n\n\n\n\n\n\nConfigure volumes\n\n\n\n\nBe sure to remove the line that is mounting your local code into the container\n\n\n\n\n\n\n\n\nConfigure your web container\n\n\n\n\n\n\nAdd an \nexpose\n section, and specify port \n\"80\"\n\n\n\n\nThis will broadcast the ports available to services like nginx-proxy\n\n\n\n\n\n\n\n\nIn the environment section, add a \nVIRTUAL_HOST\n variable. Put the hostname you want for your environment of the format \nname\n.ci.p2devcloud.com\n\n\n\n\n\n\n\n\n\n\nCreate your integration job\n\n\n\n\n\n\nCreate a job that will run every time code is committed to a particular branch (usually \ndevelop\n)\n\n\n\n\n\n\nThe script for that job should run the following commands\n\n\n\n\n\n\ndocker-compose build\n\n\n\n\n\n\ndocker-compose -f docker-compose.int.yml up -d\n\n\n\n\n\n\n\n\n\n\nAn example of this script can be seen here: \nhttp://build.p2devcloud.com/job/Fieldnotes%20Integration/configure\n\n\n\n\n\n\n\n\n\n\nInstead of creating new project tabs on http://build.p2devcloud.com, we are switching to project-specific Jenkins instances for integration. To create your own Jenkins integration server:\n\n\n\n\n\n\nGo to \nhttp://build.ci.p2devcloud.com/job/ci-start\n and run this job with the following parameters:\n\n\n\n\nYour project name (no spaces)\n\n\nThe git url for your repository\n\n\nThe branch to use\n\n\n\n\n\n\n\n\nThis will create your own Jenkins instance -- the url should appear here: http://ci.p2devcloud.com/\n\n\n\n\nYou can add/configure Jenkins plugins by adding a jenkins directory to your env directory. You can modify the config.xml files to your heart's content because the instance on our integration server is your very own configured Jenkin's instance. For an example of a env jenkins directory, checkout \nexamples/jenkins/env\n.\n\n\n\n\nMore advanced integration use cases\n\n\nThe above section highlights the most straight-forward integration implementation. There are often a collection of other things that are used when building a full featured CI environment.  We have not figured these all out, but will update this documentation as we formalize the recommended practices around such things as:\n\n\n\n\n\n\nReset to a known version of the database\n\n\n\n\n\n\nUse a version of a production database for each integration build\n\n\n\n\n\n\nBuild a production artifact\n\n\n\n\n\n\nCreate a feature branch environment for review\n\n\n\n\n\n\nCreate a content environment for migration review and manual content migrations\n\n\n\n\n\n\nProper cleanup and removing of an integration environment",
            "title": "Integration"
        },
        {
            "location": "/40_INTEGRATION/#integration-environments",
            "text": "Now that we use docker-compose we can easily spin up integration environments for our projects.  We are currently using  ci.p2devcloud.com  and  ci2.p2devcloud.com  as the integration environment. You can see a list of all integration sites currently running by going to  http://ci.p2devcloud.com  and  http://ci2.p2devcloud.com",
            "title": "Integration Environments"
        },
        {
            "location": "/40_INTEGRATION/#how-to-get-an-integration-environment-going",
            "text": "The good news is that if you have your dev environment running via docker-compose, it should be very straight-forward to pull up an integration environment.  Think of each environment you want to run as having it\u2019s own docker-compose file. See the  sample integration file  included in example/drupal8.  For a quick and easy start:    Make a copy of docker-compose.yml and name it docker-compose.int.yml    Configure volumes   Be sure to remove the line that is mounting your local code into the container     Configure your web container    Add an  expose  section, and specify port  \"80\"   This will broadcast the ports available to services like nginx-proxy     In the environment section, add a  VIRTUAL_HOST  variable. Put the hostname you want for your environment of the format  name .ci.p2devcloud.com      Create your integration job    Create a job that will run every time code is committed to a particular branch (usually  develop )    The script for that job should run the following commands    docker-compose build    docker-compose -f docker-compose.int.yml up -d      An example of this script can be seen here:  http://build.p2devcloud.com/job/Fieldnotes%20Integration/configure      Instead of creating new project tabs on http://build.p2devcloud.com, we are switching to project-specific Jenkins instances for integration. To create your own Jenkins integration server:    Go to  http://build.ci.p2devcloud.com/job/ci-start  and run this job with the following parameters:   Your project name (no spaces)  The git url for your repository  The branch to use     This will create your own Jenkins instance -- the url should appear here: http://ci.p2devcloud.com/   You can add/configure Jenkins plugins by adding a jenkins directory to your env directory. You can modify the config.xml files to your heart's content because the instance on our integration server is your very own configured Jenkin's instance. For an example of a env jenkins directory, checkout  examples/jenkins/env .",
            "title": "How to get an integration environment going"
        },
        {
            "location": "/40_INTEGRATION/#more-advanced-integration-use-cases",
            "text": "The above section highlights the most straight-forward integration implementation. There are often a collection of other things that are used when building a full featured CI environment.  We have not figured these all out, but will update this documentation as we formalize the recommended practices around such things as:    Reset to a known version of the database    Use a version of a production database for each integration build    Build a production artifact    Create a feature branch environment for review    Create a content environment for migration review and manual content migrations    Proper cleanup and removing of an integration environment",
            "title": "More advanced integration use cases"
        },
        {
            "location": "/50_TROUBLESHOOTING/",
            "text": "Troubleshooting\n\n\nSee the following sections for common problems and ways to solve them.\n\n\nRun doctor\n\n\nRun \ndevtools doctor\n to determine if your environment is set to run DevTools.\n\n\nEnsure the environment is setup correctly\n\n\nIt can be useful to ensure everything is in a clean state. The following should ensure that\n\n\n\n\ndevtools stop\n stops the docker machine and cleans up networking\n\n\neval \"$(devtools config)\"\n clears environmental variables docker uses to communicate with the docker host\n\n\ndevtools start\n starts the docker machine\n\n\neval \"$(devtools config)\"\n sets environmental variables docker uses to communicate with the docker host\n\n\n\n\nEnsure your images are up to date\n\n\nFrom time to time images are updated to fix bugs or add functionality. You won't automatically receive these updates but you can fetch them when you hear new ones are available.\n\n\ndocker pull imagename\n can be used if you want to update a specific image. For example, if you wanted to make sure you had the latest dnsdock you'd run \ndocker pull phase2/dnsdock\n.\n\n\ndocker-compose pull\n can be used within a project directory to make sure you've got the latest version of all images in the docker-compose.yml file.\n\n\nConfigure Your Shell\n\n\nIf you do not have any containers listed when running \ndocker ps\n or you get an error message like:\n\n\nGet http:///var/run/docker.sock/v1.20/containers/json: dial unix /var/run/docker.sock: no such file or directory.\n\n* Are you trying to connect to a TLS-enabled daemon without TLS?\n\n* Is your docker daemon up and running?\n\n\n\nOr an error message like:\n\n\nCouldn't connect to Docker daemon - you might need to run `boot2docker up`.\n\n\n\nMake sure your shell has the necessary environment variables by running:\n\n\neval \"$(devtools config)\"\n\n\nReset everything\n\n\nIf a problem continue to persists and the Docker Host is non-responsive, you may need to resort to the nuclear option of blowing everything away and starting over. Note this is a \nnuclear option\n as even your persistent data area will be removed if you don't back it up. If you have any data that needs to be maintained be sure to get a copy of it off of your VM first with the scripts provided.\n\n\nTo wipe everything out and start over\n\n\n\n\n\n\nFirst backup your existing data (if desired) by running \ndevtools data-backup\n. This will sync your entire \n/data\n directory to your host machine.\n\n\n\n\n\n\nThen you can run \ndevtools remove\n This removes the broken Docker Host and it\u2019s state, making way for a clean start.\n\n\n\n\n\n\nNext you can rebuild everything by running \ndevtools start\n. This will create you new Docker Host.\n\n\n\n\n\n\nIf you wish to restore the \n/data\n directory that you previously backed up, then run \ndevtools data-restore\n\n\n\n\n\n\nFiles Not Found\n\n\nIf you get messages about files not being found in the container that should be shared from your host computer, check the following:\n\n\n\n\n\n\nIs the project checked out under the \n/Users\n folder? Only files under the \n/Users\n folder are shared into the Docker Host (and thus containers) by default.\n\n\n\n\n\n\nDoes the \ndocker-compose.yml\n file have a volume mount set up that contains the missing files?\n\n\n\n\n\n\nGet shell in the Docker container via \ndocker exec -it \ncontainer name\n /bin/bash\n and check if the files are shared in the wrong place.\n\n\n\n\n\n\nGet shell in the Docker Machine with \ndocker-machine ssh dev\n and check if you find the files under /Users.\n\n\n\n\n\n\nLog in to Docker Hub\n\n\nIf you encounter the following error about the Docker image not being found when starting a project, it may indicate that your Docker client is not logged into a required private Docker Hub repository:\n\n\nPulling repository phase2/mariadb\n\nError: image phase2/mariadb:latest not found\n\n\n\nTo solve this, run \ndocker login\n and provide the relevant credentials.\n\n\nDocker client and server version incompatibilities\n\n\nIf you see an error similar to this: \n\n\nError response from daemon: client is newer than server (client API version: 1.20, server API version: 1.19)\n\n\n\nYou likely need to upgrade your Docker Machine ISO. Do that with:\n\n\n```\ndevtools stop\ndocker-machine upgrade dev\ndevtools start\n```\n\n\n\nHowever, please check in #devtools-support to ensure that upgrading your Docker Machine is the right course of action.\n\n\nNetwork timed out and can't pull container image\n\n\nExample:\n\n\nPulling cache (phase2/memcache:latest)...\nPulling repository docker.io/phase2/memcache\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/phase2/memcache/images. You may want to check your internet connection or if you are behind a proxy.\n\n\n\n\nTry restarting docker-machine: \ndevtools restart\n\n\nUpstream tracking issue",
            "title": "Troubleshooting"
        },
        {
            "location": "/50_TROUBLESHOOTING/#troubleshooting",
            "text": "See the following sections for common problems and ways to solve them.",
            "title": "Troubleshooting"
        },
        {
            "location": "/50_TROUBLESHOOTING/#run-doctor",
            "text": "Run  devtools doctor  to determine if your environment is set to run DevTools.",
            "title": "Run doctor"
        },
        {
            "location": "/50_TROUBLESHOOTING/#ensure-the-environment-is-setup-correctly",
            "text": "It can be useful to ensure everything is in a clean state. The following should ensure that   devtools stop  stops the docker machine and cleans up networking  eval \"$(devtools config)\"  clears environmental variables docker uses to communicate with the docker host  devtools start  starts the docker machine  eval \"$(devtools config)\"  sets environmental variables docker uses to communicate with the docker host",
            "title": "Ensure the environment is setup correctly"
        },
        {
            "location": "/50_TROUBLESHOOTING/#ensure-your-images-are-up-to-date",
            "text": "From time to time images are updated to fix bugs or add functionality. You won't automatically receive these updates but you can fetch them when you hear new ones are available.  docker pull imagename  can be used if you want to update a specific image. For example, if you wanted to make sure you had the latest dnsdock you'd run  docker pull phase2/dnsdock .  docker-compose pull  can be used within a project directory to make sure you've got the latest version of all images in the docker-compose.yml file.",
            "title": "Ensure your images are up to date"
        },
        {
            "location": "/50_TROUBLESHOOTING/#configure-your-shell",
            "text": "If you do not have any containers listed when running  docker ps  or you get an error message like:  Get http:///var/run/docker.sock/v1.20/containers/json: dial unix /var/run/docker.sock: no such file or directory.\n\n* Are you trying to connect to a TLS-enabled daemon without TLS?\n\n* Is your docker daemon up and running?  Or an error message like:  Couldn't connect to Docker daemon - you might need to run `boot2docker up`.  Make sure your shell has the necessary environment variables by running:  eval \"$(devtools config)\"",
            "title": "Configure Your Shell"
        },
        {
            "location": "/50_TROUBLESHOOTING/#reset-everything",
            "text": "If a problem continue to persists and the Docker Host is non-responsive, you may need to resort to the nuclear option of blowing everything away and starting over. Note this is a  nuclear option  as even your persistent data area will be removed if you don't back it up. If you have any data that needs to be maintained be sure to get a copy of it off of your VM first with the scripts provided.  To wipe everything out and start over    First backup your existing data (if desired) by running  devtools data-backup . This will sync your entire  /data  directory to your host machine.    Then you can run  devtools remove  This removes the broken Docker Host and it\u2019s state, making way for a clean start.    Next you can rebuild everything by running  devtools start . This will create you new Docker Host.    If you wish to restore the  /data  directory that you previously backed up, then run  devtools data-restore",
            "title": "Reset everything"
        },
        {
            "location": "/50_TROUBLESHOOTING/#files-not-found",
            "text": "If you get messages about files not being found in the container that should be shared from your host computer, check the following:    Is the project checked out under the  /Users  folder? Only files under the  /Users  folder are shared into the Docker Host (and thus containers) by default.    Does the  docker-compose.yml  file have a volume mount set up that contains the missing files?    Get shell in the Docker container via  docker exec -it  container name  /bin/bash  and check if the files are shared in the wrong place.    Get shell in the Docker Machine with  docker-machine ssh dev  and check if you find the files under /Users.",
            "title": "Files Not Found"
        },
        {
            "location": "/50_TROUBLESHOOTING/#log-in-to-docker-hub",
            "text": "If you encounter the following error about the Docker image not being found when starting a project, it may indicate that your Docker client is not logged into a required private Docker Hub repository:  Pulling repository phase2/mariadb\n\nError: image phase2/mariadb:latest not found  To solve this, run  docker login  and provide the relevant credentials.",
            "title": "Log in to Docker Hub"
        },
        {
            "location": "/50_TROUBLESHOOTING/#docker-client-and-server-version-incompatibilities",
            "text": "If you see an error similar to this:   Error response from daemon: client is newer than server (client API version: 1.20, server API version: 1.19)  You likely need to upgrade your Docker Machine ISO. Do that with:  ```\ndevtools stop\ndocker-machine upgrade dev\ndevtools start\n```  However, please check in #devtools-support to ensure that upgrading your Docker Machine is the right course of action.",
            "title": "Docker client and server version incompatibilities"
        },
        {
            "location": "/50_TROUBLESHOOTING/#network-timed-out-and-cant-pull-container-image",
            "text": "Example:  Pulling cache (phase2/memcache:latest)...\nPulling repository docker.io/phase2/memcache\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/phase2/memcache/images. You may want to check your internet connection or if you are behind a proxy.  Try restarting docker-machine:  devtools restart  Upstream tracking issue",
            "title": "Network timed out and can't pull container image"
        },
        {
            "location": "/60_FAQ/",
            "text": "FAQs\n\n\nViewing logs from my container service(s)?\n\n\nWhen you start a your containers via \ndocker-compose up\n all of the defined services will start in the foreground. All log output to stdout/stderr within each container will be output the console.  Each entry will be prefixed with the name of the running container to identify the source of the log message.\n\n\nIf log output is not coming directly to the console you can \ndocker exec -it \nname\n /bin/bash\n into the running container and browse the file system for logs. Most common services will provide some log output in \n/var/log\n.\n\n\nWhere do I run a build?\n\n\nYou should run a build from within the build tools container for your project.  Look at the \nexamples/drupal8/build.yml\n file to see the various way you can configure build containers for your environment.  That docker-compose file contains instructions on how you could use each of those containers for executing build commands on your codebase.\n\n\nHow do I share the output from my build container with my web container?\n\n\nFirst have your build container define a volume to mount your project code into the container, then when you perform a build it will write the output to you local project. Then if your web container defines a volume to mount the build output into the docroot you will have seamless integration of build output to web container running the site.\n\n\nHow do I configure my web container so I can run multiple projects at once from the same container?\n\n\nYou don\u2019t. Seriously. This is a place where you must change your thinking from that of treating a \nserver\n as a unit to treating the \nservice\n as a unit. If you have multiple projects active at once, you\u2019ll have a web serving container active for each of them. This is ok because containers are much lighter weight than full virtual machines, though you do want to be careful about starting too many at once.  Consider that you will have a docker-compose file for each project you are working on, and each docker-compose file represents your application and all the services required to run it.  You could be running multiple docker-compose applications on a single Docker Host.  Realistically, for the performance of your computer you would stop one docker-compose environment before you would run another.\n\n\nHow do I debug or get a shell into my application?\n\n\nThe way to run a command in a container is: \ndocker-compose run \ncontainer name\n \ncommand\n. For example, to get a shell into your web container you might run \ndocker-compose run web /bin/bash\n \n\n\nTo run a series of commands, you must wrap them in a single command using a shell. For example: \ndocker-compose run \nname in yml\n sh -c '\ncommand 1\n \n \ncommand 2\n \n \ncommand 3\n'\n\n\nIn some cases you may want to run a container that is not defined by a docker-compose.yml file, for example to test a new container configuration. Use docker run to start a new container with a given image: \ndocker run -it \nimage name\n \ncommand\n\n\nThe docker run command accepts command line options to specify volume mounts, environment variables, the working directory, and more. \n\n\nHow do I connect to an existing container that is already running\n\n\nThere is also a docker exec command that can be used to connect to a container that is already running.  \n\n\n\n\n\n\nUse \ndocker ps\n to get the name of the existing container\n\n\n\n\n\n\nUse the command \ndocker exec -it \ncontainer name\n /bin/bash\n to get a bash shell in the container\n\n\n\n\n\n\nGenerically, use \ndocker exec -it \ncontainer name\n \ncommand\n to execute whatever command you specify in the container.\n\n\n\n\n\n\nHow do I see what containers are running on my Docker Host?\n\n\nIf you want to see how many containers are present within your Docker Host VM and check on the status of them just run \ndocker ps\n This will show you the containers running, the image they are based on, the ports they expose and the name of the container.\n\n\nTo see all the containers on the Docker Host both running and stopped use the command  \ndocker ps -a\n\n\nWhy do I have so many containers / cleanup?\n\n\nAny time you finish a project or you need to \"reset\" things, you should clean up your containers for a project by running \ndocker-compose rm\n. That will remove the instances for the containers specified in your docker compose file.\n\n\nAdditionally you can run \ndevtools prune\n to clear out all stopped containers and any dangling images.\n\n\nPerformance monitoring of containers\n\n\nIf you need some insight into how many resources a given Docker container may be using, take advantage of the command \ndocker stats \ncontainer name\n.  This handy command will show you CPU%, Memory%, Memory Usage vs Limit, and Network I/O.  It is rudimentary but can be very useful in the first line of inspection on a container.\n\n\nI previously used boot2docker instead of this docker-machine. What's the difference?\n\n\nDev Tools still uses the boot2docker \nvirtual machine\n. The \"boot2docker CLI\" has been phased out in favor of docker-machine. docker-machine still uses the boot2docker linux distribution. docker-machine spins up a vm named \ndev\n in Virtualbox or VMWare Fusion, whereas boot2docker previously spun up a vm named \nboot2docker\n.",
            "title": "FAQ"
        },
        {
            "location": "/60_FAQ/#faqs",
            "text": "",
            "title": "FAQs"
        },
        {
            "location": "/60_FAQ/#viewing-logs-from-my-container-services",
            "text": "When you start a your containers via  docker-compose up  all of the defined services will start in the foreground. All log output to stdout/stderr within each container will be output the console.  Each entry will be prefixed with the name of the running container to identify the source of the log message.  If log output is not coming directly to the console you can  docker exec -it  name  /bin/bash  into the running container and browse the file system for logs. Most common services will provide some log output in  /var/log .",
            "title": "Viewing logs from my container service(s)?"
        },
        {
            "location": "/60_FAQ/#where-do-i-run-a-build",
            "text": "You should run a build from within the build tools container for your project.  Look at the  examples/drupal8/build.yml  file to see the various way you can configure build containers for your environment.  That docker-compose file contains instructions on how you could use each of those containers for executing build commands on your codebase.",
            "title": "Where do I run a build?"
        },
        {
            "location": "/60_FAQ/#how-do-i-share-the-output-from-my-build-container-with-my-web-container",
            "text": "First have your build container define a volume to mount your project code into the container, then when you perform a build it will write the output to you local project. Then if your web container defines a volume to mount the build output into the docroot you will have seamless integration of build output to web container running the site.",
            "title": "How do I share the output from my build container with my web container?"
        },
        {
            "location": "/60_FAQ/#how-do-i-configure-my-web-container-so-i-can-run-multiple-projects-at-once-from-the-same-container",
            "text": "You don\u2019t. Seriously. This is a place where you must change your thinking from that of treating a  server  as a unit to treating the  service  as a unit. If you have multiple projects active at once, you\u2019ll have a web serving container active for each of them. This is ok because containers are much lighter weight than full virtual machines, though you do want to be careful about starting too many at once.  Consider that you will have a docker-compose file for each project you are working on, and each docker-compose file represents your application and all the services required to run it.  You could be running multiple docker-compose applications on a single Docker Host.  Realistically, for the performance of your computer you would stop one docker-compose environment before you would run another.",
            "title": "How do I configure my web container so I can run multiple projects at once from the same container?"
        },
        {
            "location": "/60_FAQ/#how-do-i-debug-or-get-a-shell-into-my-application",
            "text": "The way to run a command in a container is:  docker-compose run  container name   command . For example, to get a shell into your web container you might run  docker-compose run web /bin/bash    To run a series of commands, you must wrap them in a single command using a shell. For example:  docker-compose run  name in yml  sh -c ' command 1     command 2     command 3 '  In some cases you may want to run a container that is not defined by a docker-compose.yml file, for example to test a new container configuration. Use docker run to start a new container with a given image:  docker run -it  image name   command  The docker run command accepts command line options to specify volume mounts, environment variables, the working directory, and more.",
            "title": "How do I debug or get a shell into my application?"
        },
        {
            "location": "/60_FAQ/#how-do-i-connect-to-an-existing-container-that-is-already-running",
            "text": "There is also a docker exec command that can be used to connect to a container that is already running.      Use  docker ps  to get the name of the existing container    Use the command  docker exec -it  container name  /bin/bash  to get a bash shell in the container    Generically, use  docker exec -it  container name   command  to execute whatever command you specify in the container.",
            "title": "How do I connect to an existing container that is already running"
        },
        {
            "location": "/60_FAQ/#how-do-i-see-what-containers-are-running-on-my-docker-host",
            "text": "If you want to see how many containers are present within your Docker Host VM and check on the status of them just run  docker ps  This will show you the containers running, the image they are based on, the ports they expose and the name of the container.  To see all the containers on the Docker Host both running and stopped use the command   docker ps -a",
            "title": "How do I see what containers are running on my Docker Host?"
        },
        {
            "location": "/60_FAQ/#why-do-i-have-so-many-containers-cleanup",
            "text": "Any time you finish a project or you need to \"reset\" things, you should clean up your containers for a project by running  docker-compose rm . That will remove the instances for the containers specified in your docker compose file.  Additionally you can run  devtools prune  to clear out all stopped containers and any dangling images.",
            "title": "Why do I have so many containers / cleanup?"
        },
        {
            "location": "/60_FAQ/#performance-monitoring-of-containers",
            "text": "If you need some insight into how many resources a given Docker container may be using, take advantage of the command  docker stats  container name .  This handy command will show you CPU%, Memory%, Memory Usage vs Limit, and Network I/O.  It is rudimentary but can be very useful in the first line of inspection on a container.",
            "title": "Performance monitoring of containers"
        },
        {
            "location": "/60_FAQ/#i-previously-used-boot2docker-instead-of-this-docker-machine-whats-the-difference",
            "text": "Dev Tools still uses the boot2docker  virtual machine . The \"boot2docker CLI\" has been phased out in favor of docker-machine. docker-machine still uses the boot2docker linux distribution. docker-machine spins up a vm named  dev  in Virtualbox or VMWare Fusion, whereas boot2docker previously spun up a vm named  boot2docker .",
            "title": "I previously used boot2docker instead of this docker-machine. What's the difference?"
        },
        {
            "location": "/99_LINUX/",
            "text": "Phase2 Dev Tools VM for Linux\n\n\nWhen running Docker containers on Linux, it is not necessary to run the Docker Machine VM. The instructions here describe how to run Dev Tools projects on Linux.\n\n\nLinux requirements\n\n\n\n\nThe phase2/dnsdock container, used to support automatic creation and maintenance of DNS namespace for the containers\n\n\nThis is a public container that is only about 12M; you may freely pull it\n\n\n\n\n\n\nUse of one of three options to forward DNS queries to the dnsdock container (see \nLinux DNS configuration options\n below\n\n\n\n\nLinux installation on Fedora 20 or 21\n\n\n\n\nInstall Docker\n\n\nyum install docker-io\n\n\n\n\n\n\nInstall pip\n\n\nyum install python-pip\n\n\n\n\n\n\nInstall Fig\n\n\nsudo pip install fig\n\n\n\n\n\n\nAdd your user to the Docker group\n\n\nsudo usermod -aG docker $USER\n\n\n\n\n\n\nLog out, then back in, in order to pick up your new group assignments\n\n\nSet the DNS configuration for dnsdock, as well as known RFC-1918 address space\n\n\nPlease note that the following command will over-write your existing Docker daemon configuration file.  Please set the -bip=172.17.42.1/24 and -dns=172.17.42.1 options manually as an alternative\n\n\necho 'OPTIONS=-bip=172.17.42.1/24 -dns=172.17.42.1' | sudo tee /etc/sysconfig/docker\n\n\n\n\n\n\nSet up the docker0 network as trusted\n\n\nsudo firewall-cmd --zone=trusted --add-interface=docker0 \n sudo firewall-cmd --zone=trusted --add-interface=docker0 --permanent\n\n\n\n\n\n\nStart the docker daemon\n\n\nsudo systemctl start docker\n\n\n\n\n\n\nLog into the OCHA Docker Hub Account with the credentials furnished to you.  You'll only need to do this once.\n\n\ndocker login\n\n\n\n\n\n\nPull the phase2/dnsdock container\n\n\ndocker pull phase2/dnsdock\n\n\n\n\n\n\n\n\nLinux installation on Ubuntu/Linux Mint/Debian\n\n\n\n\nInstall Docker\n\n\ncurl -sSL https://get.docker.com/ | sh\n\n\n\n\n\n\nInstall pip\n\n\napt-get install python-pip\n\n\n\n\n\n\nInstall Fig\n\n\nsudo pip install docker-compose\n\n\n\n\n\n\nAdd your user to the Docker group\n\n\nsudo usermod -aG docker $USER\n\n\n\n\n\n\nLog out, then back in, in order to pick up your new group assignments\n\n\nSet the DNS configuration for dnsdock, as well as known RFC-1918 address space\n\n\nPlease note that the following command will over-write your existing Docker daemon configuration file.  Please set the -bip=172.17.42.1/24 and -dns=172.17.42.1 options manually as an alternative\n\n\necho 'OPTIONS=-bip=172.17.42.1/24 -dns=172.17.42.1' | sudo tee /etc/default/docker\n\n\n\n\n\n\nStart the docker daemon\n\n\nsudo start docker\n\n\n\n\n\n\nPull the phase2/dnsdock container\n\n\ndocker pull phase2/dnsdock\n\n\n\n\n\n\n\n\nLinux DNS configuration options\n\n\nMethod 1: dnsmasq via NetworkManager\n\n\nThis method works well with no other needed software provided that you have unfettered access to your system's configuration, and are using NetworkManager to maintain your networking stack\n\n\n\n\nAdd the line dns=dnsmasq to /etc/NetworkManager/NetworkManager.conf under the [main] configuration stanza. This will cause NetworkManager to spawn and use a dnsmasq process for all name resolution.\nIf you already have a local configuration, ensure that it is not configured to start on system boot.\n\n\nAdd a single rule to direct all DNS lookups for .vm addresses to the 172.17.42.1 address.\n\n\necho 'server=/vm/172.17.42.1' | sudo tee /etc/NetworkManager/dnsmasq.d/dnsdock.conf\n\n\n\n\n\n\nRestart NetworkManager, either through systemd, or by simply rebooting.  To restart via systemd:\n\n\nsystemctl restart NetworkManager\n\n\n\n\n\n\nRun the dnsdock container\n\n\ndocker run -d --name=dnsdock -e DNSDOCK_NAME=dnsdock -e DNSDOCK_IMAGE=devtools -p 172.17.42.1:53:53/udp -v /var/run/docker.sock:/var/run/docker.sock phase2/dnsdock\n\n\n\n\n\n\n\n\nMethod 2: dnsdock as main resolver\n\n\nThis method will probably only work well if this is a fixed computer or server with a consistent single upstream DNS server. If you meet these criteria, you can very easily use this to set up .vm resolution for containers an delegate the rest to your normal DNS server.\n\n\nThis example assumes that the upstream DNS server for a Linux workstation is 192.168.0.1.\n\n\n\n\nRun the dnsdock container, specifying your upstream DNS server at the end.\n\n\ndocker run -d --name=dnsdock -e DNSDOCK_NAME=dnsdock -e DNSDOCK_IMAGE=devtools -p 172.17.42.1:53:53/udp -v /var/run/docker.sock:/var/run/docker.sock phase2/dnsdock /opt/bin/dnsdock -domain=vm -nameserver='192.168.0.1:53'\n\n\n\n\n\n\nConfigure 172.17.42.1 as your first DNS resolver in your network configuration. The method for doing this may differ based on whether you are using a desktop environment or running Linux on a server, but that nameserver should end up as the first 'nameserver' line in your /etc/resolv.conf file.\n\n\n\n\nMethod 3: libnss-resolver\n\n\nlibnss-resolver is an app that adds Mac-style /etc/resolver/$FQDN files to the Linux NSS resolution stack to query a different DNS server for any .vm address.  It may be the easiest option for most installations.\n\n\nThere are releases for Fedora 20, Ubuntu 12.04 and Ubuntu 14.04.\n\n\n\n\nInstall libnss-resolver from https://github.com/azukiapp/libnss-resolver/releases\n\n\nSet up .vm hostname resolution\n\n\necho 'nameserver 172.17.42.1:53' | sudo tee /etc/resolver/vm\n\n\n\n\n\n\nRun the dnsdock container\n\n\ndocker run -d --name=dnsdock -e DNSDOCK_NAME=dnsdock -e DNSDOCK_IMAGE=devtools -p 172.17.42.1:53:53/udp -v /var/run/docker.sock:/var/run/docker.sock phase2/dnsdock\n\n\n\n\n\n\n\n\nDNS resolution tests\n\n\nOnce you have your environment set up, you can use the following tests to ensure things are running properly.\n\n\n\n\ndig @172.17.42.1 dnsdock.devtools.vm.\n\n\nYou should get a 172.17.0.0/16 address.\n\n\n\n\n\n\nping dnsdock.devtools.vm\n\n\nYou should get echo replies from a 172.17.0.0/16 address.\n\n\n\n\n\n\ngetent hosts dnsdock.devtools.vm\n\n\nYou should get a 172.17.0.0/16 address.",
            "title": "LINUX"
        },
        {
            "location": "/99_LINUX/#phase2-dev-tools-vm-for-linux",
            "text": "When running Docker containers on Linux, it is not necessary to run the Docker Machine VM. The instructions here describe how to run Dev Tools projects on Linux.",
            "title": "Phase2 Dev Tools VM for Linux"
        },
        {
            "location": "/99_LINUX/#linux-requirements",
            "text": "The phase2/dnsdock container, used to support automatic creation and maintenance of DNS namespace for the containers  This is a public container that is only about 12M; you may freely pull it    Use of one of three options to forward DNS queries to the dnsdock container (see  Linux DNS configuration options  below",
            "title": "Linux requirements"
        },
        {
            "location": "/99_LINUX/#linux-installation-on-fedora-20-or-21",
            "text": "Install Docker  yum install docker-io    Install pip  yum install python-pip    Install Fig  sudo pip install fig    Add your user to the Docker group  sudo usermod -aG docker $USER    Log out, then back in, in order to pick up your new group assignments  Set the DNS configuration for dnsdock, as well as known RFC-1918 address space  Please note that the following command will over-write your existing Docker daemon configuration file.  Please set the -bip=172.17.42.1/24 and -dns=172.17.42.1 options manually as an alternative  echo 'OPTIONS=-bip=172.17.42.1/24 -dns=172.17.42.1' | sudo tee /etc/sysconfig/docker    Set up the docker0 network as trusted  sudo firewall-cmd --zone=trusted --add-interface=docker0   sudo firewall-cmd --zone=trusted --add-interface=docker0 --permanent    Start the docker daemon  sudo systemctl start docker    Log into the OCHA Docker Hub Account with the credentials furnished to you.  You'll only need to do this once.  docker login    Pull the phase2/dnsdock container  docker pull phase2/dnsdock",
            "title": "Linux installation on Fedora 20 or 21"
        },
        {
            "location": "/99_LINUX/#linux-installation-on-ubuntulinux-mintdebian",
            "text": "Install Docker  curl -sSL https://get.docker.com/ | sh    Install pip  apt-get install python-pip    Install Fig  sudo pip install docker-compose    Add your user to the Docker group  sudo usermod -aG docker $USER    Log out, then back in, in order to pick up your new group assignments  Set the DNS configuration for dnsdock, as well as known RFC-1918 address space  Please note that the following command will over-write your existing Docker daemon configuration file.  Please set the -bip=172.17.42.1/24 and -dns=172.17.42.1 options manually as an alternative  echo 'OPTIONS=-bip=172.17.42.1/24 -dns=172.17.42.1' | sudo tee /etc/default/docker    Start the docker daemon  sudo start docker    Pull the phase2/dnsdock container  docker pull phase2/dnsdock",
            "title": "Linux installation on Ubuntu/Linux Mint/Debian"
        },
        {
            "location": "/99_LINUX/#linux-dns-configuration-options",
            "text": "",
            "title": "Linux DNS configuration options"
        },
        {
            "location": "/99_LINUX/#method-1-dnsmasq-via-networkmanager",
            "text": "This method works well with no other needed software provided that you have unfettered access to your system's configuration, and are using NetworkManager to maintain your networking stack   Add the line dns=dnsmasq to /etc/NetworkManager/NetworkManager.conf under the [main] configuration stanza. This will cause NetworkManager to spawn and use a dnsmasq process for all name resolution.\nIf you already have a local configuration, ensure that it is not configured to start on system boot.  Add a single rule to direct all DNS lookups for .vm addresses to the 172.17.42.1 address.  echo 'server=/vm/172.17.42.1' | sudo tee /etc/NetworkManager/dnsmasq.d/dnsdock.conf    Restart NetworkManager, either through systemd, or by simply rebooting.  To restart via systemd:  systemctl restart NetworkManager    Run the dnsdock container  docker run -d --name=dnsdock -e DNSDOCK_NAME=dnsdock -e DNSDOCK_IMAGE=devtools -p 172.17.42.1:53:53/udp -v /var/run/docker.sock:/var/run/docker.sock phase2/dnsdock",
            "title": "Method 1: dnsmasq via NetworkManager"
        },
        {
            "location": "/99_LINUX/#method-2-dnsdock-as-main-resolver",
            "text": "This method will probably only work well if this is a fixed computer or server with a consistent single upstream DNS server. If you meet these criteria, you can very easily use this to set up .vm resolution for containers an delegate the rest to your normal DNS server.  This example assumes that the upstream DNS server for a Linux workstation is 192.168.0.1.   Run the dnsdock container, specifying your upstream DNS server at the end.  docker run -d --name=dnsdock -e DNSDOCK_NAME=dnsdock -e DNSDOCK_IMAGE=devtools -p 172.17.42.1:53:53/udp -v /var/run/docker.sock:/var/run/docker.sock phase2/dnsdock /opt/bin/dnsdock -domain=vm -nameserver='192.168.0.1:53'    Configure 172.17.42.1 as your first DNS resolver in your network configuration. The method for doing this may differ based on whether you are using a desktop environment or running Linux on a server, but that nameserver should end up as the first 'nameserver' line in your /etc/resolv.conf file.",
            "title": "Method 2: dnsdock as main resolver"
        },
        {
            "location": "/99_LINUX/#method-3-libnss-resolver",
            "text": "libnss-resolver is an app that adds Mac-style /etc/resolver/$FQDN files to the Linux NSS resolution stack to query a different DNS server for any .vm address.  It may be the easiest option for most installations.  There are releases for Fedora 20, Ubuntu 12.04 and Ubuntu 14.04.   Install libnss-resolver from https://github.com/azukiapp/libnss-resolver/releases  Set up .vm hostname resolution  echo 'nameserver 172.17.42.1:53' | sudo tee /etc/resolver/vm    Run the dnsdock container  docker run -d --name=dnsdock -e DNSDOCK_NAME=dnsdock -e DNSDOCK_IMAGE=devtools -p 172.17.42.1:53:53/udp -v /var/run/docker.sock:/var/run/docker.sock phase2/dnsdock",
            "title": "Method 3: libnss-resolver"
        },
        {
            "location": "/99_LINUX/#dns-resolution-tests",
            "text": "Once you have your environment set up, you can use the following tests to ensure things are running properly.   dig @172.17.42.1 dnsdock.devtools.vm.  You should get a 172.17.0.0/16 address.    ping dnsdock.devtools.vm  You should get echo replies from a 172.17.0.0/16 address.    getent hosts dnsdock.devtools.vm  You should get a 172.17.0.0/16 address.",
            "title": "DNS resolution tests"
        }
    ]
}